{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T15:07:46.763800Z","iopub.status.busy":"2023-04-02T15:07:46.762424Z","iopub.status.idle":"2023-04-02T15:08:15.212815Z","shell.execute_reply":"2023-04-02T15:08:15.211193Z","shell.execute_reply.started":"2023-04-02T15:07:46.763741Z"},"id":"PrAoByo9PvDQ","trusted":true},"outputs":[],"source":["!pip install ultralytics\n","!pip install supervision\n","# Importing required libraries\n","from IPython import display\n","from ultralytics import YOLO\n","import supervision as sv\n","import cv2\n","import numpy as np\n","from datetime import timedelta\n","import os\n","from matplotlib import rcParams\n","from matplotlib import animation, rc\n","from IPython.display import HTML\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","display.clear_output()"]},{"cell_type":"markdown","metadata":{"id":"gj6QJnN4C2_O"},"source":[" **Video Information Extraction**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T15:08:23.318346Z","iopub.status.busy":"2023-04-02T15:08:23.317648Z","iopub.status.idle":"2023-04-02T15:08:23.325845Z","shell.execute_reply":"2023-04-02T15:08:23.324739Z","shell.execute_reply.started":"2023-04-02T15:08:23.318305Z"},"id":"FxYlPHyfHocK","trusted":true},"outputs":[],"source":["def get_video_info(video_path):\n","    \n","    # Extracting information about the video\n","    video_info = sv.VideoInfo.from_video_path(video_path)\n","    width, height, fps, total_frames = video_info.width, video_info.height, video_info.fps, video_info.total_frames\n","    \n","    # Calculating the length of the video by dividing the total number of frames by the frame rate and rounding to the nearest second\n","    video_length = timedelta(seconds = round(total_frames / fps))\n","    \n","    # Print out the video resolution, fps, and length u\n","    print(f\"\\033[1mVideo Resolution:\\033[0m ({width}, {height})\")\n","    print(f\"\\033[1mFPS:\\033[0m {fps}\")\n","    print(f\"\\033[1mLength:\\033[0m {video_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-02T15:08:41.936226Z","iopub.status.busy":"2023-04-02T15:08:41.935837Z","iopub.status.idle":"2023-04-02T15:08:41.983493Z","shell.execute_reply":"2023-04-02T15:08:41.981858Z","shell.execute_reply.started":"2023-04-02T15:08:41.936190Z"},"id":"KmzMRm3hIVku","outputId":"7e4a1217-d131-4cb7-bc32-1918024a4f53","trusted":true},"outputs":[],"source":["# Extracting information of the test video\n","get_video_info('vehicle-count-2/Data/Example Videos/test1.mp4')"]},{"cell_type":"markdown","metadata":{"id":"P7MGRwewI7R2"},"source":["**Line Coordinates**"]},{"cell_type":"markdown","metadata":{"id":"hgok8pJZKgDo"},"source":["\n","\n"," **website [PolygoneZone](https://roboflow.github.io/polygonzone/](https://roboflow.github.io/polygonzone/).** This website allowed us to easily draw a polygon on the image and get the coordinates for that polygon."]},{"cell_type":"markdown","metadata":{"id":"aCtLPMlwM6Sm"},"source":["**Vehicle Count**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T15:08:51.561045Z","iopub.status.busy":"2023-04-02T15:08:51.560584Z","iopub.status.idle":"2023-04-02T15:08:51.575678Z","shell.execute_reply":"2023-04-02T15:08:51.574103Z","shell.execute_reply.started":"2023-04-02T15:08:51.560982Z"},"id":"8xH1UmOK8wbk","trusted":true},"outputs":[],"source":["def vehicle_count(source_path, destination_path, line_start, line_end):\n","   \n","   # Load the pre-trained YOLO model\n","   model = YOLO('yolov8l.pt')\n","\n","   # Create two points from the line_start and line_end coordinates\n","   line_start = sv.Point(line_start[0], line_start[1])\n","   line_end = sv.Point(line_end[0], line_end[1])\n","   \n","   # Create a line zone object from the two points\n","   line_counter = sv.LineZone(line_start, line_end) \n","   \n","   # Create a line zone annotator object with specific thickness and text scale\n","   line_annotator = sv.LineZoneAnnotator(thickness = 2,\n","                                         text_thickness = 1,\n","                                         text_scale = 0.5)\n","   \n","   # Create a box annotator object with specific thickness and text scale\n","   box_annotator = sv.BoxAnnotator(thickness = 1,\n","                                   text_thickness = 1,\n","                                   text_scale = 0.5)\n","   \n","   # Extract information about the video from the given source path\n","   video_info = sv.VideoInfo.from_video_path(source_path)\n","\n","   # Create a video out path by combining the destination path and the video name\n","   video_name = os.path.splitext(os.path.basename(source_path))[0] + \".mp4\"\n","   video_out_path = os.path.join(destination_path, video_name)\n","\n","   # Create a video writer object for the output video\n","   video_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), video_info.fps, (video_info.width, video_info.height))\n","   \n","   # Loop over each frame of the video and perform object detection and tracking\n","   for result in model.track(source = source_path, tracker = 'bytetrack.yaml', show=True, stream=True, agnostic_nms=True):\n","        \n","        # Get the original frame from the detection result\n","        frame = result.orig_img\n","        \n","        # Convert the YOLO detection results to a Detections object\n","        detections = sv.Detections.from_yolov8(result)\n","      \n","        # If the detections have an associated ID, set the tracker ID in the Detections object\n","        if result.boxes.id is not None:\n","          detections.tracker_id = result.boxes.id.cpu().numpy().astype(int)\n","\n","        # Filter the detections to only include classes 2 (cars) and 7 (trucks)\n","        detections = detections[(detections.class_id == 2) | (detections.class_id == 7)]\n","\n","        # Generate labels for the detections, including the tracker ID, class name, and confidence\n","        labels = [f\"{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n","                  for _, confidence, class_id, tracker_id \n","                  in detections]\n","    \n","        # Trigger the line counter to count any detections that intersect the line zone\n","        line_counter.trigger(detections)\n","     \n","        # Annotate the frame with the line zone and any intersecting detections\n","        line_annotator.annotate(frame, line_counter)\n","\n","        # Annotate the frame with bounding boxes and labels for all detections\n","        frame = box_annotator.annotate(scene = frame,\n","                                       detections = detections,\n","                                       labels = labels)\n","        # Write the annotated frame to the output video\n","        video_out.write(frame)\n","  \n","   # Release the video writer and clear the Jupyter Notebook output\n","   video_out.release()\n","   display.clear_output()"]},{"cell_type":"markdown","metadata":{"id":"JFW3FZv3hoVW"},"source":["**Usage**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-02T15:10:00.421386Z","iopub.status.busy":"2023-04-02T15:10:00.420990Z","iopub.status.idle":"2023-04-02T15:22:00.594890Z","shell.execute_reply":"2023-04-02T15:22:00.593139Z","shell.execute_reply.started":"2023-04-02T15:10:00.421356Z"},"id":"5mT0kAHOVKrn","trusted":true},"outputs":[],"source":["# Performing vehicle count on the test video\n","vehicle_count(source_path = \"vehicle-count-2/Data/Example Videos/test1.mp4\",\n","              destination_path = \"vehicle-count-2/Data/Example Results\",\n","              line_start = (337, 391),\n","              line_end = (917, 387))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693},"execution":{"iopub.execute_input":"2023-04-02T15:38:25.819573Z","iopub.status.busy":"2023-04-02T15:38:25.819147Z","iopub.status.idle":"2023-04-02T15:38:38.157146Z","shell.execute_reply":"2023-04-02T15:38:38.154220Z","shell.execute_reply.started":"2023-04-02T15:38:25.819538Z"},"id":"1CSlbtRmaoAD","outputId":"b8a2af7f-144e-4a7c-819f-1511515c733f","trusted":true},"outputs":[],"source":["# Set the embed limit to 50 MB\n","rcParams['animation.embed_limit'] = 50.0\n","\n","# Read the GIF image and extract its frames\n","gif_path = 'vehicle-count-2/Data/Example Results/test1.gif'\n","gif = Image.open(gif_path)\n","frames = []\n","for i in range(gif.n_frames):\n","    gif.seek(i)\n","    frame = gif.copy()\n","    frames.append(frame)\n","\n","# Create the animation function\n","def create_animation(ims):\n","    \n","    fig = plt.figure(figsize=(10, 6))\n","    plt.axis('off')\n","    im = plt.imshow(ims[0])\n","    plt.close()\n","    \n","    def animate_func(i):\n","        im.set_array(ims[i])\n","        return [im]\n","\n","    return animation.FuncAnimation(fig, animate_func, frames=len(ims), interval=1000//4)\n","\n","# Create the animation object\n","animation_obj = create_animation(frames)\n","\n","# Display the animation in the notebook\n","HTML(animation_obj.to_jshtml())"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}
